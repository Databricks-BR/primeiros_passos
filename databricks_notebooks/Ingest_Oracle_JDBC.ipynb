{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb089dd0-ae76-44ca-8953-2b6ce4840ca9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Passo 1 - Instalação do DRIVER JDBC\n",
    "\n",
    "* Install the Databricks JDBC driver in a Java project\n",
    "* https://docs.databricks.com/integrations/jdbc-odbc-bi.html#jdbc-driver\n",
    "* https://www.databricks.com/spark/jdbc-drivers-download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2329894d-86b6-419b-8cf6-423d91e1c848",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Passo 2 - Leitura usando o JDBC (spark.read)\n",
    "\n",
    "##### Referência:\n",
    "* https://docs.databricks.com/external-data/jdbc.html#query-databases-using-jdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bc1da5c-2d11-4ff6-9576-5998014f59c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:oracle:thin:username/password@//hostname:portnumber/SID\") \\\n",
    "    .option(\"dbtable\", \"hr.emp\") \\\n",
    "    .option(\"user\", \"db_user_name\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e80550a-210e-4925-b012-a1aa8a470e36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Use fetchsize to boost reading speed\n",
    "Yet another JDBC parameter which controls the number of rows fetched per iteration from a remote JDBC database.\n",
    "It defaults to low fetch size (e.g. Oracle with 10 rows).\n",
    "\n",
    "Aumentar para 100 reduz o número total de consultas que precisam ser executadas por um fator de 10. Os resultados do JDBC são tráfego de rede, portanto, evite números muito grandes, mas os valores ideais podem estar na casa dos milhares para muitos conjuntos de dados.\n",
    "\n",
    "##### Referência:\n",
    "* https://docs.databricks.com/external-data/jdbc.html#control-number-of-rows-fetched-per-query\n",
    "* https://luminousmen.com/post/spark-tips-optimizing-jdbc-data-source-reads\n",
    "* https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcfbffdf-6aba-4ffc-bb30-c0c1b8718fca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "\t.format(\"jdbc\") \\\n",
    "\t.option(\"url\", \"jdbc:oracle:thin:username/password@//hostname:portnumber/SID\") \\\n",
    "    .option(\"dbtable\", \"db.table\") \\\n",
    "\t.option(\"user\", \"user\")\\\n",
    "\t.option(\"password\", \"pass\") \\\n",
    "\t.option(\"fetchsize\",\"100\") \\\n",
    "\t.option(\"queryTimeout\",\"0\") \\\n",
    "\t.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a7d2da4-5f64-4216-a546-2238c1999fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Query Mode\n",
    "\n",
    "##### Referência:\n",
    "* https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_Oracle_JDBC_Howto.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbff32a-6bd0-49ba-a03c-d5d09ac3b5a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "db_user = \"system\"\n",
    "db_connect_string = \"localhost:1521/XEPDB1\" // dbserver:port/service_name\n",
    "db_pass = \"oracle\"\n",
    "myquery = \"select rownum as id from dual connect by level<=10\"\n",
    "\n",
    "df = spark.read.format(\"jdbc\").\n",
    "           option(\"url\", s\"jdbc:oracle:thin:@$db_connect_string\").\n",
    "           option(\"driver\", \"oracle.jdbc.driver.OracleDriver\").\n",
    "           option(\"query\", myquery).\n",
    "           // option(\"dbtable\", \"(select * ....)\"). // enclosing the query in parenthesis it's like query mode\n",
    "           // option(\"dbtable\", \"myschema.mytable\"). // use this to simply extract a given table \n",
    "           option(\"user\", db_user).\n",
    "           option(\"password\", db_pass).\n",
    "           option(\"fetchsize\", 10000).\n",
    "           load()\n",
    "\n",
    "df.printSchema\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c8b5d66-4186-4095-b490-7afd4aa410fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Loading Data from Autonomous Database\n",
    "\n",
    "##### Referência:\n",
    "* https://docs.oracle.com/en-us/iaas/data-flow/using/spark_oracle_ds_examples.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a11f5e-6fc9-4d3e-8964-efeab6f0ccc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading data from autonomous database at root compartment.\n",
    "# Note you don't have to provide driver class name and jdbc url.\n",
    "\n",
    "oracle_df = spark.read \\\n",
    "    .format(\"oracle\") \\\n",
    "    .option(\"adbId\",\"ocid1.autonomousdatabase.<REALM>.[REGION][.FUTURE USE].<UNIQUE ID>\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0722f8-9ebe-471e-b7c2-3c2eb3b753b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://learn.microsoft.com/pt-br/azure/databricks/getting-started/dataframes-python#save-a-dataframe-to-a-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5dd5ab3-c0c9-4f16-91e0-0fa3fc091e6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.write.saveAsTable(\"<table_name>\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Ingest_Oracle_JDBC",
   "notebookOrigID": 3538949529803379,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
